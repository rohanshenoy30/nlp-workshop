{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11192596,
          "sourceType": "datasetVersion",
          "datasetId": 6987299
        },
        {
          "sourceId": 11192623,
          "sourceType": "datasetVersion",
          "datasetId": 6987319
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangQuest-2025"
      ],
      "metadata": {
        "id": "Ctypg1V8CxfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Regex"
      ],
      "metadata": {
        "id": "gvvulVxMCxfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pN2J4MtnCxfJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "chat1 = '12345678912, abc@xyz.com, 9810235533'\n",
        "chat2 = '(123)-567-8912, abc_80@xyz.com, 9810235533'\n",
        "chat3 = 'yes, phone : 12345678912, email: abc@xyz.com'"
      ],
      "metadata": {
        "id": "B1WV7UHFCxfL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'(\\(\\d{3}\\)-\\d{3}-\\d{4})|\\d{10}'\n",
        "matches = re.findall(pattern, chat3)\n",
        "matches"
      ],
      "metadata": {
        "id": "pGJrNlQaCxfM",
        "outputId": "007a374a-72fe-4be6-f5da-0dc8154d13cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ? marks the preceeding character or group optional (0 or 1 occurance) and '.' matches nay single character except a newline(\\n)"
      ],
      "metadata": {
        "id": "rW-pI5MKCxfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'(\\(?\\d{3}\\)?-?\\d{3}-?\\d{4})'\n",
        "matches = re.findall(pattern, chat2)\n",
        "matches"
      ],
      "metadata": {
        "id": "SKfGD-r2CxfP",
        "outputId": "0999fb8c-c538-4ba3-abb6-a95e1bce5f7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['(123)-567-8912', '9810235533']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### For email id:\n",
        "##### (*) means as many occurances as possible (can be zero as well)"
      ],
      "metadata": {
        "id": "RlsZ4umVCxfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pat = r'[a-zA-Z0-9_]*@[a-zA-Z]*\\.[a-zA-Z]*'\n",
        "matches = re.findall(pat, chat2)\n",
        "matches"
      ],
      "metadata": {
        "id": "5dT1ouZ5CxfS",
        "outputId": "4d224b03-82cb-4ad4-adee-166bb9bc9a91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['abc_80@xyz.com']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### to match non didgit or special characters(^)"
      ],
      "metadata": {
        "id": "hpHUA6n4CxfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat1 = '#123456'\n",
        "chat2 = '.123456'\n",
        "chat3 = '/234567'"
      ],
      "metadata": {
        "id": "2xKXPmo6CxfV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pat = r'[^\\d]*\\d*'\n",
        "mat = re.findall(pat, chat1)\n",
        "mat"
      ],
      "metadata": {
        "id": "oagGzGGECxfY",
        "outputId": "7bfcfbdf-d74e-48d9-910d-f067eb1c4d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['#123456', '']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pat = r'[^\\d]*(\\d*)'\n",
        "mat = re.findall(pat, chat1)\n",
        "mat"
      ],
      "metadata": {
        "id": "t9muElP6CxfZ",
        "outputId": "3c15e72f-72b8-4cfb-9da7-59dc2d6b5567"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['123456', '']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "to match exactly what you're looking for in text use parenthesis to specify() in the following way:"
      ],
      "metadata": {
        "id": "8ga4vjdWCxfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = 'age 50'\n",
        "pat = r'age \\d+'\n",
        "mat = re.findall(pat, chat)\n",
        "mat"
      ],
      "metadata": {
        "id": "AOUrVlNkCxfa",
        "outputId": "ab2cd47c-f763-407b-d671-e45b50f92273"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['age 50']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "to only extract the age (number)"
      ],
      "metadata": {
        "id": "Xh83BcN0Cxfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = 'age 50'\n",
        "pat = r'age (\\d+)'\n",
        "mat = re.findall(pat, chat)\n",
        "mat"
      ],
      "metadata": {
        "id": "goxPwWZ7Cxfc",
        "outputId": "3251c2ca-5fde-4cef-cef8-b16f191f56db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['50']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization using Spacy\n",
        "1. Word Tokenization: breaking down a sentence into words\n",
        "2. Sentence Tokenization: breaking down a paragraph into sentences"
      ],
      "metadata": {
        "id": "KNfo0OAzCxfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "zupXion8Cxfd"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### using a blank pipeline"
      ],
      "metadata": {
        "id": "JbrlXJUfCxfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blank pipeline always has a word tokenizer by default so we son't need to add any extra attributes in it"
      ],
      "metadata": {
        "id": "a1x50gUfCxfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenization\n",
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Dr. Sharma is a surgeon. He is really good at what he does.\")\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "L7t-eHJECxfe",
        "outputId": "f43388ef-d075-4ec2-98ad-246c219273c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr.\n",
            "Sharma\n",
            "is\n",
            "a\n",
            "surgeon\n",
            ".\n",
            "He\n",
            "is\n",
            "really\n",
            "good\n",
            "at\n",
            "what\n",
            "he\n",
            "does\n",
            ".\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''Dayton high school, 8th grade students information\n",
        "========================================================\n",
        "\n",
        "Name         birthday         email\n",
        "-----        ---------        ------\n",
        "Virat        5 June, 1882     virat@kohli.com\n",
        "Maria        12 April, 2001   maria@sharapova.com\n",
        "Serena       24 June, 1998    serena@williams.com\n",
        "Joe          1 May, 1997      joe@root.com\n",
        "'''"
      ],
      "metadata": {
        "id": "jpPSG3UVCxff"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting emails without using regex using (.like_email)\n",
        "> Note: REGEX can also be used but Spacy makes your work much more easier in some cases"
      ],
      "metadata": {
        "id": "elHL0vxzCxff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "emails = [token.text for token in doc if token.like_email]\n",
        "\n",
        "print(emails)"
      ],
      "metadata": {
        "id": "xyV9OTgLCxfg",
        "outputId": "8713d20a-e8da-47e3-cd21-37cbb16b0e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['virat@kohli.com', 'maria@sharapova.com', 'serena@williams.com', 'joe@root.com']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "l2eQpsNWCxfg",
        "outputId": "372c315f-278c-47bd-d0d6-3528ead87bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dayton\n",
            "high\n",
            "school\n",
            ",\n",
            "8th\n",
            "grade\n",
            "students\n",
            "information\n",
            "\n",
            "\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "=\n",
            "\n",
            "\n",
            "\n",
            "Name\n",
            "        \n",
            "birthday\n",
            "        \n",
            "email\n",
            "\n",
            "\n",
            "-----\n",
            "       \n",
            "---------\n",
            "       \n",
            "------\n",
            "\n",
            "\n",
            "Virat\n",
            "       \n",
            "5\n",
            "June\n",
            ",\n",
            "1882\n",
            "    \n",
            "virat@kohli.com\n",
            "\n",
            "\n",
            "Maria\n",
            "       \n",
            "12\n",
            "April\n",
            ",\n",
            "2001\n",
            "  \n",
            "maria@sharapova.com\n",
            "\n",
            "\n",
            "Serena\n",
            "      \n",
            "24\n",
            "June\n",
            ",\n",
            "1998\n",
            "   \n",
            "serena@williams.com\n",
            "\n",
            "\n",
            "Joe\n",
            "         \n",
            "1\n",
            "May\n",
            ",\n",
            "1997\n",
            "     \n",
            "joe@root.com\n",
            "\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a blank pipeline if we try to perform sentence tokenization in spacy that is not possible hence we can either add sentencizer in the blank pipeline as shown below or simply use a pretrained pipeline using the following command:**\n",
        "> nlp = spacy.load(\"en_core_web\")"
      ],
      "metadata": {
        "id": "ZnaEDzExCxfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n"
      ],
      "metadata": {
        "id": "idopac_QF50u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "text = nlp(\"Dr. Strange loves India. Hulk loves Delhi.\")\n",
        "for sent in text.sents:\n",
        "    print(sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkNTQ_37Cxfh",
        "outputId": "15ce25d3-2946-4c96-fc8d-631648aa4ce7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr. Strange loves India.\n",
            "Hulk loves Delhi.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cannot be used for sentence tokenization**"
      ],
      "metadata": {
        "id": "k0sVdjr3Cxfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentencizer = nlp.add_pipe(\"sentencizer\")  # Add a simple sentence segmenter in the blank pipeline\n",
        "doc = nlp(\"This is sentence one. This is sentence two!\")\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)"
      ],
      "metadata": {
        "id": "wV_29_0wCxfi",
        "outputId": "d98748de-5776-4ed2-fe5b-bda11a084df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is sentence one.\n",
            "This is sentence two!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank('en')\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "JcrqGFVOCxfk",
        "outputId": "75aff68d-c55c-49c5-f12f-0c198ea602c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Use a built existing pipeline:**\n"
      ],
      "metadata": {
        "id": "pKn4YoItCxfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(r\"en_core_web_sm\")\n",
        "text = nlp(\"Dr.Strange loves India. Hulk loves Delhi.\")\n",
        "for sent in text.sents:\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "_K8kVPcoCxfl",
        "outputId": "f7da10f5-17ac-4a4a-c60e-76cfe75bee83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dr.Strange loves India.\n",
            "Hulk loves Delhi.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "9c2258BWCxfm",
        "outputId": "6f36fad1-5523-4006-a818-ec5ac0481646"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline"
      ],
      "metadata": {
        "id": "uVvZhaQjCxfm",
        "outputId": "d2900632-f10b-44de-bf4b-b81bd1d2af54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x24681b2ecf0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x24681b2dfd0>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x24680d69930>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x246819f0d10>),\n",
              " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x246ffd5bb90>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x24680d6bbc0>)]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Stemming:\n",
        "1. talking -> talk\n",
        "2. eating -> eat\n",
        "3. ate -> eat\n",
        "4. adjustable -> adjust**\n",
        "\n",
        "**basically uses fixed rules such as remove able, ing etc to derive a base word\n",
        "LEMMATIZATION:\n",
        "uses knowledge of language (linguistic knowledge) to derive a base word\n",
        "ability -> ability in lemmatization but\n",
        "ability -> abil in stemming**"
      ],
      "metadata": {
        "id": "LU6gghsOCxfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**as you can notice words like ate and ability aren't being converted to their true base forms so the need for a more advanced technique called lemmatization arises**"
      ],
      "metadata": {
        "id": "SIiCISsLCxfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = ['eating', 'eats', 'eat', 'ate', 'adjustable', 'rafting', 'ability', 'meeting']\n",
        "for word in words:\n",
        "    print(word, \" | \", stemmer.stem(word))"
      ],
      "metadata": {
        "id": "CNdTk5IrCxfo",
        "outputId": "fbdea98e-ba51-4217-b1c4-722db44bc349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating  |  eat\n",
            "eats  |  eat\n",
            "eat  |  eat\n",
            "ate  |  ate\n",
            "adjustable  |  adjust\n",
            "rafting  |  raft\n",
            "ability  |  abil\n",
            "meeting  |  meet\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Lemmatization:\n",
        "> **using the pretrained nlp pipeline for lemmatization using the command (.lemma_) all words are converted to their true base forms**"
      ],
      "metadata": {
        "id": "UzjEPdQbCxfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(r\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "UvezWVOECxfq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"eating eats eat ate adjustable rafting ability eating better\")\n",
        "for token in doc:\n",
        "    print(token, \" | \", token.lemma_)"
      ],
      "metadata": {
        "id": "G1Kome2lCxfr",
        "outputId": "076f86a3-03c4-4fb0-9330-28d3d245ed2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eating  |  eat\n",
            "eats  |  eat\n",
            "eat  |  eat\n",
            "ate  |  eat\n",
            "adjustable  |  adjustable\n",
            "rafting  |  rafting\n",
            "ability  |  ability\n",
            "eating  |  eat\n",
            "better  |  well\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('Mando talked for 3 ghours although talking isn\\'t his thing he became talkative')\n",
        "for token in doc:\n",
        "    print(token, \"|\", token.lemma_)"
      ],
      "metadata": {
        "id": "07Oe1VclCxfr",
        "outputId": "ca887df6-0536-4e68-beda-cef5e5133c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mando | Mando\n",
            "talked | talk\n",
            "for | for\n",
            "3 | 3\n",
            "ghours | ghour\n",
            "although | although\n",
            "talking | talk\n",
            "is | be\n",
            "n't | not\n",
            "his | his\n",
            "thing | thing\n",
            "he | he\n",
            "became | become\n",
            "talkative | talkative\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Part of Speech Tagging"
      ],
      "metadata": {
        "id": "hlAYCC6lCxfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Captain America ate 100$ of samosa. Then he said I can do this all day.\")\n",
        "for token in doc:\n",
        "    print(token, ' | ', token.pos_)"
      ],
      "metadata": {
        "id": "SurTLKHCCxfs",
        "outputId": "db4d56d1-93b3-415f-b8bd-61fdb9f92507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Captain  |  PROPN\n",
            "America  |  PROPN\n",
            "ate  |  VERB\n",
            "100  |  NUM\n",
            "$  |  NUM\n",
            "of  |  ADP\n",
            "samosa  |  PROPN\n",
            ".  |  PUNCT\n",
            "Then  |  ADV\n",
            "he  |  PRON\n",
            "said  |  VERB\n",
            "I  |  PRON\n",
            "can  |  AUX\n",
            "do  |  VERB\n",
            "this  |  PRON\n",
            "all  |  DET\n",
            "day  |  NOUN\n",
            ".  |  PUNCT\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "to see the tense of the word you can use tag_ fucntion"
      ],
      "metadata": {
        "id": "JAgd_Fk-Cxft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
        "for token in doc:\n",
        "    print(token, \"|\", token.pos_, '|', spacy.explain(token.pos_),\n",
        "          \"|\", token.tag_, \"|\", spacy.explain(token.tag_))"
      ],
      "metadata": {
        "id": "yWjzDK5iCxft",
        "outputId": "c0ac29bb-7722-473a-f76b-9b0554c981ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wow | INTJ | interjection | UH | interjection\n",
            "! | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
            "Dr. | PROPN | proper noun | NNP | noun, proper singular\n",
            "Strange | PROPN | proper noun | NNP | noun, proper singular\n",
            "made | VERB | verb | VBD | verb, past tense\n",
            "265 | NUM | numeral | CD | cardinal number\n",
            "million | NUM | numeral | CD | cardinal number\n",
            "$ | NUM | numeral | CD | cardinal number\n",
            "on | ADP | adposition | IN | conjunction, subordinating or preposition\n",
            "the | DET | determiner | DT | determiner\n",
            "very | ADV | adverb | RB | adverb\n",
            "first | ADJ | adjective | JJ | adjective (English), other noun-modifier (Chinese)\n",
            "day | NOUN | noun | NN | noun, singular or mass\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2VEC Textual Representation**\n"
      ],
      "metadata": {
        "id": "wJA4GwKYE9Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn42Gq3eIOap",
        "outputId": "52ed01a5-c429-4f72-daed-c8d72a78dc44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "sentence = \"TensorQuest is an exciting machine learning competition\"\n",
        "\n",
        "doc = nlp(sentence.lower())\n",
        "\n",
        "word_vectors = {token.text: token.vector for token in doc}\n",
        "\n",
        "print(\"Vector representation of 'machine':\\n\", word_vectors['machine'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBsFv9jcFQ3m",
        "outputId": "f333f25f-6ec3-4869-a11d-bddd32abf97a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'machine':\n",
            " [-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204    0.023277\n",
            "  0.1297    -0.2112     0.32876    0.67447    0.10047   -0.30559\n",
            "  0.11213    0.22959   -0.32997    0.1389    -0.57289    2.523\n",
            " -0.32921    0.06045    0.23895    0.1091     0.19358   -0.1765\n",
            "  0.11583    0.63204   -0.13644   -0.24354    0.20061   -0.50244\n",
            "  0.40537   -0.38688    0.73784    0.093937  -0.30643    0.045874\n",
            "  0.097915  -0.082114   0.13082   -0.039022   0.088084  -0.27023\n",
            " -0.077658  -0.0045355  0.18986   -0.063083  -0.138      0.40474\n",
            " -0.16199   -0.10953    0.22923   -0.67634   -0.65763   -0.044595\n",
            " -0.12119    0.071167   0.25993   -0.27052   -0.22474   -0.13818\n",
            "  0.20692    0.87604   -0.35257   -0.1498     0.72804    0.68768\n",
            "  0.19993    0.084733  -0.2234     0.11301    0.29895   -0.090119\n",
            "  0.038172  -0.32912    0.014221  -0.36335    0.5898     0.10467\n",
            "  0.16549    0.47199    0.078939  -0.19985    0.84014   -0.2277\n",
            " -0.22907   -0.26243   -0.32598    1.0146    -0.079235  -0.34248\n",
            "  0.032767   0.49757    0.0047373  0.057762   0.19319    0.10756\n",
            "  0.16938    0.42513   -0.22691    0.095343  -0.094303  -0.3849\n",
            " -0.27853   -0.4554     0.2735    -2.1344     0.040352   0.065232\n",
            "  0.23147    0.13766    0.41638    0.1153     0.61593   -0.012896\n",
            "  0.22778    0.13342    0.12902   -0.0054822 -0.55536    0.56218\n",
            " -0.082096  -0.46214    0.21512   -0.09482   -0.16694   -0.94924\n",
            "  0.08068    0.64041    0.0089012  0.043188  -0.030859   0.02754\n",
            "  0.12671   -0.2175     0.0098018  0.14784    0.37847   -0.32479\n",
            " -0.21623    0.14108    0.069705  -0.18381    0.10896   -0.12666\n",
            " -0.063817  -0.44837   -0.079542  -0.54454    0.32602   -0.089619\n",
            " -0.069878   0.010953  -0.16017   -0.13361    0.37901    0.74596\n",
            " -0.01211   -0.4469     0.16831    0.12325   -0.29108   -0.6984\n",
            " -0.33013   -0.20759   -0.070089   0.023017  -0.32895   -0.02034\n",
            "  0.50521   -0.12432    0.26341   -0.063996  -0.46205   -0.39595\n",
            " -0.15154   -0.22158   -0.050627  -0.015164  -0.38784    0.5011\n",
            "  0.19628   -0.31646   -0.48555   -0.49464    0.35255   -0.060035\n",
            "  0.082212   0.084107   0.17729   -0.55179    0.071874  -0.39032\n",
            "  0.40137   -0.2273     0.35788    0.42503   -0.20496    0.58632\n",
            " -0.2015     0.35892    0.045149  -0.20252    0.15502   -0.019122\n",
            " -0.11768   -0.48471    0.35088    0.14332    0.091038   0.28448\n",
            "  0.35166   -0.87305    0.047971   0.43431   -0.34814    0.035735\n",
            " -0.21673    0.062818  -0.40837   -0.21775    0.1597     0.56172\n",
            " -0.11126    0.33851   -0.31825   -0.10671    0.057792  -0.03997\n",
            " -0.15047   -0.11435    0.56779    0.43056   -0.17674   -0.045657\n",
            "  0.04453   -0.11629    0.094277  -0.46008   -0.12373   -0.18918\n",
            "  0.14565   -0.17643    0.45186   -0.011373   0.16214   -0.17391\n",
            " -0.14195    0.14683   -0.055814   0.52315    0.0032239 -0.51676\n",
            " -0.094159   0.21092    0.22586   -0.78028   -0.67057   -0.031255\n",
            "  0.045657   0.033926   0.16709    0.014854  -0.14456   -0.56059\n",
            " -0.26709   -0.17691    0.15472    0.46348    1.4026    -0.24662\n",
            "  0.3447    -0.56387    0.047655  -0.39049   -0.016389   0.52322\n",
            " -0.22908   -0.31134    0.6579    -0.67904    0.40002    0.055284\n",
            "  0.5956     0.031032  -0.19315   -0.65318   -0.28911    0.13658\n",
            " -0.42426   -0.37742   -0.17644   -0.15885    0.48907    1.0195\n",
            "  0.12087   -0.36731    0.0087637  0.10666   -0.12636    0.66767  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Named Entity Recognititon (NER)"
      ],
      "metadata": {
        "id": "ecXQ5RnhCxfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**identifies and classifies entities like people, organizations, locations, dates, and more, within unstructured text.**\n"
      ],
      "metadata": {
        "id": "d934xRkzCxfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc. is going to acquire twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ' | ', ent.label_ , \" | \", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "id": "7EQ1aphRCxfu",
        "outputId": "5e9f793f-51a8-4816-a45c-d63d63fced90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla Inc.  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification Model"
      ],
      "metadata": {
        "id": "mZgbs785Cxfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tGymSMYCCxfw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r'C:\\Users\\Maanya Verma\\Downloads\\bbc-text.csv\\bbc-text.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "454Cj_KlCxfw",
        "outputId": "11b3b918-a9c9-4663-e4f0-b80c5689a5ac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>business</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>politics</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>politics</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>sport</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "0              tech  tv future in the hands of viewers with home th...\n",
              "1          business  worldcom boss  left books alone  former worldc...\n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3             sport  yeading face newcastle in fa cup premiership s...\n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
              "...             ...                                                ...\n",
              "2220       business  cars pull down us retail figures us retail sal...\n",
              "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
              "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
              "2223       politics  how political squabbles snowball it s become c...\n",
              "2224          sport  souness delight at euro progress boss graeme s...\n",
              "\n",
              "[2225 rows x 2 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoding the category column"
      ],
      "metadata": {
        "id": "ugZcbzbvCxfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"category_encoded\"], category_mapping = pd.factorize(df[\"category\"])\n",
        "print(category_mapping)\n"
      ],
      "metadata": {
        "id": "RASCBIhtCxfy",
        "outputId": "5a86687e-27d4-4f89-a581-0a4a38205f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['tech', 'business', 'sport', 'entertainment', 'politics'], dtype='object')\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "cYA9kiC3Cxfz",
        "outputId": "5f6c1fc4-d6f1-47ad-a33a-e553bfcaf43f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>category_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>business</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>politics</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>politics</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>sport</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text  \\\n",
              "0              tech  tv future in the hands of viewers with home th...   \n",
              "1          business  worldcom boss  left books alone  former worldc...   \n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
              "3             sport  yeading face newcastle in fa cup premiership s...   \n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
              "...             ...                                                ...   \n",
              "2220       business  cars pull down us retail figures us retail sal...   \n",
              "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
              "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
              "2223       politics  how political squabbles snowball it s become c...   \n",
              "2224          sport  souness delight at euro progress boss graeme s...   \n",
              "\n",
              "      category_encoded  \n",
              "0                    0  \n",
              "1                    1  \n",
              "2                    2  \n",
              "3                    2  \n",
              "4                    3  \n",
              "...                ...  \n",
              "2220                 1  \n",
              "2221                 4  \n",
              "2222                 3  \n",
              "2223                 4  \n",
              "2224                 2  \n",
              "\n",
              "[2225 rows x 3 columns]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "GIxPb38eCxfz",
        "outputId": "3703a26c-fe46-4591-b9cb-7adf126efda3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2225, 3)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will now take a look at how the Product categories are mapped and then also create couple dictionaries from the same for future reference**"
      ],
      "metadata": {
        "id": "9_8TvAPICxf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_id_df = df[['category', 'category_encoded']].drop_duplicates().sort_values(by = 'category_encoded').reset_index(drop = 1)\n",
        "category_id_df"
      ],
      "metadata": {
        "id": "Iz4xzAzsCxf1",
        "outputId": "b76e6331-2ff5-4cbd-b3fb-6413dcc9940a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>category_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>politics</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category  category_encoded\n",
              "0           tech                 0\n",
              "1       business                 1\n",
              "2          sport                 2\n",
              "3  entertainment                 3\n",
              "4       politics                 4"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "8jYMJ-2eCxf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When it comes to NLP, getting rid of stopwords is one of the most important steps. It ensures we get rid of the most frequent but usually useless words, e.g. \"the\", \"a\", \"an\", etc. to eliminate any bias they might cause.\n",
        "We have many methods to eliminate stop words - many NLP libraries like sklearn have their own stop words but it is usually considered a good idea to use stop words from the NLTK library. We shall do the same.**"
      ],
      "metadata": {
        "id": "oKP_gLKtCxf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(r\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "k3PXOkS7Cxf2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())  # Convert to lowercase and process with spaCy\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)  # Return cleaned text\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "Ys5A_TwICxf2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does:\n",
        "1. text.lower()\n",
        "Converts all text to lowercase to ensure consistency (e.g., \"Apple\" and \"apple\" are treated the same).\n",
        "\n",
        "2. doc = nlp(text.lower())\n",
        "Passes the text through spaCy's NLP pipeline, which performs:\n",
        "\n",
        "Tokenization → Splits text into words.\n",
        "\n",
        "POS tagging → Identifies parts of speech (nouns, verbs, etc.).\n",
        "\n",
        "Lemmatization → Converts words to their base form (e.g., \"running\" → \"run\").\n",
        "\n",
        "Stopword detection → Identifies common words like \"the,\" \"is,\" \"and.\"\n",
        "\n",
        "3. token.lemma_ → Gets the lemma (base form) of each word.\n",
        "\n",
        "if not token.is_stop → Removes stopwords (e.g., \"the,\" \"and,\" \"is\").\n",
        "\n",
        "if not token.is_punct → Removes punctuation marks.\n",
        "\n"
      ],
      "metadata": {
        "id": "efcvrr2lCxf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "HixEZzgpCxf4",
        "outputId": "8f6babef-6ace-4b03-f994-c393c6adff0b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>category_encoded</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "      <td>0</td>\n",
              "      <td>tv future hand viewer home theatre system   pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "      <td>1</td>\n",
              "      <td>worldcom boss   leave book   worldcom boss ber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "      <td>2</td>\n",
              "      <td>tiger wary farrell   gamble   leicester rush m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "      <td>2</td>\n",
              "      <td>yeade face newcastle fa cup premiership newcas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "      <td>3</td>\n",
              "      <td>ocean s raid box office ocean s   crime caper ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>business</td>\n",
              "      <td>cars pull down us retail figures us retail sal...</td>\n",
              "      <td>1</td>\n",
              "      <td>car pull retail figure retail sale fall 0.3 ja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>politics</td>\n",
              "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
              "      <td>4</td>\n",
              "      <td>kilroy unveil immigration policy ex chatshow h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>rem announce new glasgow concert us band rem h...</td>\n",
              "      <td>3</td>\n",
              "      <td>rem announce new glasgow concert band rem anno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>politics</td>\n",
              "      <td>how political squabbles snowball it s become c...</td>\n",
              "      <td>4</td>\n",
              "      <td>political squabble snowball s commonplace argu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>sport</td>\n",
              "      <td>souness delight at euro progress boss graeme s...</td>\n",
              "      <td>2</td>\n",
              "      <td>souness delight euro progress boss graeme soun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text  \\\n",
              "0              tech  tv future in the hands of viewers with home th...   \n",
              "1          business  worldcom boss  left books alone  former worldc...   \n",
              "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
              "3             sport  yeading face newcastle in fa cup premiership s...   \n",
              "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
              "...             ...                                                ...   \n",
              "2220       business  cars pull down us retail figures us retail sal...   \n",
              "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
              "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
              "2223       politics  how political squabbles snowball it s become c...   \n",
              "2224          sport  souness delight at euro progress boss graeme s...   \n",
              "\n",
              "      category_encoded                                         clean_text  \n",
              "0                    0  tv future hand viewer home theatre system   pl...  \n",
              "1                    1  worldcom boss   leave book   worldcom boss ber...  \n",
              "2                    2  tiger wary farrell   gamble   leicester rush m...  \n",
              "3                    2  yeade face newcastle fa cup premiership newcas...  \n",
              "4                    3  ocean s raid box office ocean s   crime caper ...  \n",
              "...                ...                                                ...  \n",
              "2220                 1  car pull retail figure retail sale fall 0.3 ja...  \n",
              "2221                 4  kilroy unveil immigration policy ex chatshow h...  \n",
              "2222                 3  rem announce new glasgow concert band rem anno...  \n",
              "2223                 4  political squabble snowball s commonplace argu...  \n",
              "2224                 2  souness delight euro progress boss graeme soun...  \n",
              "\n",
              "[2225 rows x 4 columns]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "SaGO3LUdCxf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "AY2_WuNSCxf5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"clean_text\"])  # Convert text into TF-IDF features\n",
        "y = df[\"category\"]  # Target labels\n"
      ],
      "metadata": {
        "id": "Ciu22retCxf5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "rLSl3UD-Cxf5",
        "outputId": "9b73caed-6c56-4e9b-cec5-e74166b536b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2225, 23352)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X[:5, :5]"
      ],
      "metadata": {
        "id": "1WyHrVRyCxf6",
        "outputId": "c07fd39d-817b-4c12-da99-33e07f995b00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<5x5 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 2 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "LTV2LrzgCxf6",
        "outputId": "f4815a07-5492-484a-85c0-5c21bc5471a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 14425)\t0.041945871300314255\n",
            "  (0, 8852)\t0.02676235457041553\n",
            "  (0, 10984)\t0.023134929761185002\n",
            "  (0, 17433)\t0.033190033434919185\n",
            "  (0, 14137)\t0.03020988013540774\n",
            "  (0, 15835)\t0.03008699910638888\n",
            "  (0, 22904)\t0.03537450107129553\n",
            "  (0, 21145)\t0.0625378658149778\n",
            "  (0, 4047)\t0.021022622725503065\n",
            "  (0, 15733)\t0.040430913842023644\n",
            "  (0, 19740)\t0.03209161559275845\n",
            "  (0, 12057)\t0.04583939162678847\n",
            "  (0, 16535)\t0.03039790812720782\n",
            "  (0, 2014)\t0.024801402350642918\n",
            "  (0, 9118)\t0.04457129151119338\n",
            "  (0, 3188)\t0.02725532589810651\n",
            "  (0, 4590)\t0.021975115945528475\n",
            "  (0, 13909)\t0.033190033434919185\n",
            "  (0, 1107)\t0.03479625256605947\n",
            "  (0, 17872)\t0.05538824708072302\n",
            "  (0, 8958)\t0.040204730350824294\n",
            "  (0, 17665)\t0.04539339686530647\n",
            "  (0, 11220)\t0.04681601260588735\n",
            "  (0, 17356)\t0.04140753482112253\n",
            "  (0, 66)\t0.0262612629546789\n",
            "  :\t:\n",
            "  (2224, 16224)\t0.051274360662524915\n",
            "  (2224, 22899)\t0.08260332559989264\n",
            "  (2224, 2920)\t0.12973302469506673\n",
            "  (2224, 21900)\t0.06912933786675873\n",
            "  (2224, 5850)\t0.14022003983142126\n",
            "  (2224, 14742)\t0.0886169489922068\n",
            "  (2224, 20184)\t0.15946046335037278\n",
            "  (2224, 16761)\t0.08125076264827746\n",
            "  (2224, 20985)\t0.1354332202540927\n",
            "  (2224, 4871)\t0.06698502578694226\n",
            "  (2224, 9428)\t0.04033279298025083\n",
            "  (2224, 9613)\t0.053239573575458304\n",
            "  (2224, 11732)\t0.06195183158914597\n",
            "  (2224, 6140)\t0.05732386567973514\n",
            "  (2224, 3524)\t0.06973630630345759\n",
            "  (2224, 7565)\t0.04728914401322492\n",
            "  (2224, 9256)\t0.14809937711813126\n",
            "  (2224, 1406)\t0.04078416352505105\n",
            "  (2224, 18505)\t0.06467989305530224\n",
            "  (2224, 22574)\t0.08631290318546521\n",
            "  (2224, 16261)\t0.04517823845391602\n",
            "  (2224, 17352)\t0.10849883060064493\n",
            "  (2224, 23201)\t0.05412152241890749\n",
            "  (2224, 14348)\t0.0659467074738889\n",
            "  (2224, 8996)\t0.05844746420158185\n",
            "0                tech\n",
            "1            business\n",
            "2               sport\n",
            "3               sport\n",
            "4       entertainment\n",
            "            ...      \n",
            "2220         business\n",
            "2221         politics\n",
            "2222    entertainment\n",
            "2223         politics\n",
            "2224            sport\n",
            "Name: category, Length: 2225, dtype: object\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split"
      ],
      "metadata": {
        "id": "N2dDkbOOCxf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "kameyErmCxf7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection & Training (Naive Bayes)"
      ],
      "metadata": {
        "id": "uTN-IwZrCxf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "dtzS9_4MCxf8",
        "outputId": "73d3685f-5265-4c6c-c062-d767b2110e15"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1cGMLk8nCxf9",
        "outputId": "489c1564-f1b7-4709-c0db-6532be151778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9595505617977528\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     business       0.94      0.95      0.95       101\n",
            "entertainment       1.00      0.89      0.94        81\n",
            "     politics       0.92      0.98      0.95        83\n",
            "        sport       0.99      1.00      0.99        98\n",
            "         tech       0.95      0.98      0.96        82\n",
            "\n",
            "     accuracy                           0.96       445\n",
            "    macro avg       0.96      0.96      0.96       445\n",
            " weighted avg       0.96      0.96      0.96       445\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "gpYdBErbCxf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries and loading the dataset"
      ],
      "metadata": {
        "id": "-DrgCyK7Cxf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:44:39.442047Z",
          "iopub.execute_input": "2025-03-28T05:44:39.442429Z",
          "iopub.status.idle": "2025-03-28T05:44:39.446018Z",
          "shell.execute_reply.started": "2025-03-28T05:44:39.442403Z",
          "shell.execute_reply": "2025-03-28T05:44:39.445147Z"
        },
        "id": "AM1ZbWMZCxf-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/sentimenttest1/sentimenttest(1).csv.xls\")  # Load review dataset\n",
        "df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:02.623191Z",
          "iopub.execute_input": "2025-03-28T05:45:02.623550Z",
          "iopub.status.idle": "2025-03-28T05:45:02.644855Z",
          "shell.execute_reply.started": "2025-03-28T05:45:02.623524Z",
          "shell.execute_reply": "2025-03-28T05:45:02.644168Z"
        },
        "id": "rQHmQsV3Cxf-",
        "outputId": "6d94cba8-4aa3-4a82-ed87-8c68524ce758"
      },
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                               review\n0            I love this product! It works perfectly.\n1         Absolutely terrible experience. Do not buy!\n2   Great quality and fast shipping. Highly recomm...\n3       The material feels cheap and broke in a week.\n4   Decent product for the price. Not the best but...\n5       Amazing! I'm really happy with this purchase.\n6    Horrible customer service. Took ages to respond.\n7                  Super comfortable and looks great!\n8      The battery dies too quickly. Expected better.\n9                    Very satisfied. Would buy again.\n10  The packaging was damaged, but the product was...\n11  Not what I expected. The description is mislea...\n12          Solid performance and good build quality.\n13           Didn't work as advertised. Disappointed.\n14                  Perfect for my needs! Five stars.\n15  The size is too small. Runs smaller than expec...\n16               Fast delivery and excellent product.\n17                Not great, but not terrible either.\n18               Exceeded my expectations! Fantastic!\n19                    Arrived late and missing parts.\n20               Beautiful design, but feels fragile.\n21           The software is buggy and crashes often.\n22                      Reliable and worth the money.\n23                Meh, it's okay but nothing special.\n24            Best purchase I've made in a long time!\n25            Flimsy and poorly made. Broke in a day.\n26                  Very user-friendly and intuitive.\n27                  Too expensive for what it offers.\n28    Great customer support. Helped me fix an issue.\n29          Does exactly what it says. No complaints.\n30                     The buttons are hard to press.\n31                    Love it! Would buy another one.\n32  Instructions were unclear, had to figure it ou...\n33                     The sound quality is terrible.\n34                   Fantastic! Better than expected.\n35                   Cheaply made and arrived broken.\n36               Works as expected. No issues so far.\n37              I regret buying this. Waste of money.\n38                Very lightweight and easy to carry.\n39                         Decent but has some flaws.\n40             Superb performance. Worth every penny!\n41        The colors are different from the pictures.\n42            It’s okay, but I wouldn’t buy it again.\n43             A must-have product. Highly recommend!\n44                   The app keeps crashing. Useless.\n45                         Great value for the price!\n46        Shipping was slow, but the product is good.\n47                 Just average, nothing outstanding.\n48      Fantastic build quality and works flawlessly.\n49              The worst product I have ever bought.",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I love this product! It works perfectly.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Absolutely terrible experience. Do not buy!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Great quality and fast shipping. Highly recomm...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The material feels cheap and broke in a week.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Decent product for the price. Not the best but...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Amazing! I'm really happy with this purchase.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Horrible customer service. Took ages to respond.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Super comfortable and looks great!</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The battery dies too quickly. Expected better.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Very satisfied. Would buy again.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The packaging was damaged, but the product was...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Not what I expected. The description is mislea...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Solid performance and good build quality.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Didn't work as advertised. Disappointed.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Perfect for my needs! Five stars.</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The size is too small. Runs smaller than expec...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Fast delivery and excellent product.</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Not great, but not terrible either.</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Exceeded my expectations! Fantastic!</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Arrived late and missing parts.</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Beautiful design, but feels fragile.</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>The software is buggy and crashes often.</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Reliable and worth the money.</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Meh, it's okay but nothing special.</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Best purchase I've made in a long time!</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Flimsy and poorly made. Broke in a day.</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Very user-friendly and intuitive.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Too expensive for what it offers.</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Great customer support. Helped me fix an issue.</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Does exactly what it says. No complaints.</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>The buttons are hard to press.</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Love it! Would buy another one.</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Instructions were unclear, had to figure it ou...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>The sound quality is terrible.</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Fantastic! Better than expected.</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Cheaply made and arrived broken.</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Works as expected. No issues so far.</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>I regret buying this. Waste of money.</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Very lightweight and easy to carry.</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Decent but has some flaws.</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Superb performance. Worth every penny!</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>The colors are different from the pictures.</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>It’s okay, but I wouldn’t buy it again.</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>A must-have product. Highly recommend!</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>The app keeps crashing. Useless.</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>Great value for the price!</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Shipping was slow, but the product is good.</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Just average, nothing outstanding.</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Fantastic build quality and works flawlessly.</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>The worst product I have ever bought.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load a Pre-trained Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "EZ6VPAnECxf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We use \"nlptown/bert-base-multilingual-uncased-sentiment\", a BERT-based model that predicts sentiment on a scale of 1 to 5.\n",
        "* AutoTokenizer.from_pretrained(model_name): Loads the corresponding tokenizer to process text into tokenized input.\n",
        "* AutoModelForSequenceClassification.from_pretrained(model_name): Loads the pre-trained BERT model fine-tuned for sentiment classification."
      ],
      "metadata": {
        "id": "P32tvudVCxf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:14.310315Z",
          "iopub.execute_input": "2025-03-28T05:45:14.310614Z",
          "iopub.status.idle": "2025-03-28T05:45:14.803291Z",
          "shell.execute_reply.started": "2025-03-28T05:45:14.310595Z",
          "shell.execute_reply": "2025-03-28T05:45:14.802570Z"
        },
        "id": "32nzlZoyCxf_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Move Model to GPU (If Available)"
      ],
      "metadata": {
        "id": "MEOjNKMGCxgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Checks if a GPU (CUDA) is available. If yes, it uses GPU; otherwise, it falls back to CPU.\n",
        "* Moves the model to the selected device to optimize computation speed."
      ],
      "metadata": {
        "id": "_LRU388jCxgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:22.372138Z",
          "iopub.execute_input": "2025-03-28T05:45:22.372461Z",
          "iopub.status.idle": "2025-03-28T05:45:22.656024Z",
          "shell.execute_reply.started": "2025-03-28T05:45:22.372431Z",
          "shell.execute_reply": "2025-03-28T05:45:22.654994Z"
        },
        "id": "odY8RlPfCxgC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Tokenize the Review Text"
      ],
      "metadata": {
        "id": "MesWS11ZCxgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Parameters:\n",
        "\n",
        "* .tolist(): Converts the Pandas column (df[\"review\"]) to a Python list for processing.\n",
        "* add_special_tokens=True: Adds [CLS] (start) and [SEP] (end) tokens required by BERT.\n",
        "* max_length=128: Limits the review length to 128 tokens (truncates longer texts and pads shorter ones).\n",
        "* padding=\"max_length\": Ensures all inputs are padded to the same length (128 tokens).\n",
        "* return_attention_mask=True: Generates an attention mask to differentiate real tokens from padding.\n",
        "* return_tensors=\"pt\": Converts everything into PyTorch tensors for processing.\n",
        "* truncation=True: Cuts off longer texts beyond 128 tokens."
      ],
      "metadata": {
        "id": "GkVFh8AdCxgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = tokenizer(\n",
        "    df[\"review\"].tolist(),   # Convert DataFrame column to list\n",
        "    add_special_tokens=True, # Add [CLS] and [SEP] tokens\n",
        "    max_length=128,          # Truncate or pad to 128 tokens\n",
        "    padding=\"max_length\",\n",
        "    return_attention_mask=True,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:32.781150Z",
          "iopub.execute_input": "2025-03-28T05:45:32.781492Z",
          "iopub.status.idle": "2025-03-28T05:45:32.790862Z",
          "shell.execute_reply.started": "2025-03-28T05:45:32.781464Z",
          "shell.execute_reply": "2025-03-28T05:45:32.790004Z"
        },
        "id": "9AH1zqkYCxgD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encodings[\"input_ids\"].to(device)\n",
        "attention_mask = encodings[\"attention_mask\"].to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:41.365799Z",
          "iopub.execute_input": "2025-03-28T05:45:41.366115Z",
          "iopub.status.idle": "2025-03-28T05:45:41.369762Z",
          "shell.execute_reply.started": "2025-03-28T05:45:41.366092Z",
          "shell.execute_reply": "2025-03-28T05:45:41.369039Z"
        },
        "id": "6UfqjazgCxgD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Model Predictions"
      ],
      "metadata": {
        "id": "Pa57leriCxgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* torch.no_grad(): Disables gradient computation (saves memory & speeds up inference).\n",
        "* outputs = model(...): Passes tokenized input through the BERT model.\n",
        "* logits = outputs.logits: Extracts logits (raw model outputs before applying softmax)."
      ],
      "metadata": {
        "id": "7fTD1VnOCxgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:46.821143Z",
          "iopub.execute_input": "2025-03-28T05:45:46.821499Z",
          "iopub.status.idle": "2025-03-28T05:45:46.834847Z",
          "shell.execute_reply.started": "2025-03-28T05:45:46.821460Z",
          "shell.execute_reply": "2025-03-28T05:45:46.834137Z"
        },
        "id": "tD6PQ71MCxgF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"predicted_sentiment\"] = logits.argmax(axis=1).cpu().numpy() + 1\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:53.035532Z",
          "iopub.execute_input": "2025-03-28T05:45:53.035837Z",
          "iopub.status.idle": "2025-03-28T05:45:53.040308Z",
          "shell.execute_reply.started": "2025-03-28T05:45:53.035817Z",
          "shell.execute_reply": "2025-03-28T05:45:53.039657Z"
        },
        "id": "em3_0i11CxgF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"review\", \"predicted_sentiment\"]]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:45:58.374790Z",
          "iopub.execute_input": "2025-03-28T05:45:58.375110Z",
          "iopub.status.idle": "2025-03-28T05:45:58.380015Z",
          "shell.execute_reply.started": "2025-03-28T05:45:58.375083Z",
          "shell.execute_reply": "2025-03-28T05:45:58.378933Z"
        },
        "id": "eoacX7-bCxgG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"predicted_reviews.csv\", index=False)\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T05:46:14.449794Z",
          "iopub.execute_input": "2025-03-28T05:46:14.450078Z",
          "iopub.status.idle": "2025-03-28T05:46:14.456525Z",
          "shell.execute_reply.started": "2025-03-28T05:46:14.450058Z",
          "shell.execute_reply": "2025-03-28T05:46:14.455771Z"
        },
        "id": "YUpEN_nVCxgH",
        "outputId": "cb8a2200-4666-4fc5-d471-eaa518f23aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "                                              review  predicted_sentiment\n0           I love this product! It works perfectly.                    5\n1        Absolutely terrible experience. Do not buy!                    1\n2  Great quality and fast shipping. Highly recomm...                    5\n3      The material feels cheap and broke in a week.                    2\n4  Decent product for the price. Not the best but...                    3\n5      Amazing! I'm really happy with this purchase.                    5\n6   Horrible customer service. Took ages to respond.                    1\n7                 Super comfortable and looks great!                    5\n8     The battery dies too quickly. Expected better.                    2\n9                   Very satisfied. Would buy again.                    5\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}